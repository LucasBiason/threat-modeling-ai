{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 – Comparativo Gemini vs OpenAI vs Qwen (Fallback)\n",
        "\n",
        "Executa **3 LLMs** nas imagens `diagram01.png` e `diagram02.png`:\n",
        "\n",
        "| Ordem | Modelo | Papel |\n",
        "|-------|--------|-------|\n",
        "| 1 | **Gemini** (gemini-1.5-pro) | Principal |\n",
        "| 2 | **OpenAI** (gpt-4o) | Secundário (se Gemini falhar) |\n",
        "| 3 | **Qwen** (qwen2.5vl via Ollama) | Emergência (se OpenAI falhar) |\n",
        "\n",
        "Comparativo visual dos bbox e métricas para verificar qual performa melhor.\n",
        "\n",
        "**Ollama (Qwen):** `make ollama-up` e `ollama pull qwen2.5:7b-vl` (ou equivalente vision)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup e variáveis de ambiente\n",
        "\n",
        "Carregamos as chaves de API do `.env` e definimos os parâmetros (modelos, temperatura, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK Ordem (principal → emergência): ['Gemini', 'OpenAI', 'Ollama/qwen2.5vl']\n",
            "OK Gemini: True | OpenAI: True | Qwen (Ollama): sempre disponível\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "import re\n",
        "import hashlib\n",
        "import asyncio\n",
        "from pathlib import Path\n",
        "from dotenv import dotenv_values\n",
        "\n",
        "for p in [Path(\"notebooks/.env\"), Path(\".env\"), Path(\"configs/.env\")]:\n",
        "    if p.exists():\n",
        "        for k, v in dotenv_values(p).items():\n",
        "            if v:\n",
        "                os.environ[k] = v\n",
        "        break\n",
        "\n",
        "CONFIG = {\n",
        "    \"google_api_key\": os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\"),\n",
        "    \"openai_api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
        "    \"gemini_model\": \"gemini-1.5-pro\",\n",
        "    \"openai_model\": \"gpt-4o\",\n",
        "    \"ollama_base_url\": \"http://localhost:11434\",\n",
        "    \"ollama_model\": \"qwen2.5vl\",  # única LLM local — emergência\n",
        "    \"ollama_timeout\": 60,\n",
        "    \"temperature\": 0.0,\n",
        "}\n",
        "\n",
        "ASSETS_DIR = Path(\"assets\") if Path(\"assets\").exists() else Path(\"notebooks/assets\")\n",
        "\n",
        "# Ordem de fallback: Gemini (principal) → OpenAI (secundário) → Qwen (emergência)\n",
        "MODELS_TO_RUN = []\n",
        "if CONFIG[\"google_api_key\"]:\n",
        "    MODELS_TO_RUN.append(\"Gemini\")\n",
        "if CONFIG[\"openai_api_key\"]:\n",
        "    MODELS_TO_RUN.append(\"OpenAI\")\n",
        "MODELS_TO_RUN.append(\"Ollama/qwen2.5vl\")\n",
        "\n",
        "print(f\"OK Ordem (principal → emergência): {MODELS_TO_RUN}\")\n",
        "print(f\"OK Gemini: {bool(CONFIG['google_api_key'])} | OpenAI: {bool(CONFIG['openai_api_key'])} | Qwen (Ollama): sempre disponível\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. O prompt do agente de diagrama\n",
        "\n",
        "O prompt instrui a LLM a retornar um JSON estruturado com componentes, conexões e trust boundaries. É o mesmo usado no backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt (primeiras linhas):\n",
            "\n",
            "Analyze this architecture diagram.\n",
            "\n",
            "1. Identify all components (Users, Servers, Databases, Gateways, Load Balancers, etc.).\n",
            "2. Identify the connections and data flows between them.\n",
            "3. Identify trust boundaries (e.g., VPCs, Public/Private subnets, DMZs).\n",
            "\n",
            "Return ONLY a valid JSON object structured as:\n",
            "{\n",
            "  \"model\": \"model_name\",\n",
            "  \"components\": [{\"i ...\n"
          ]
        }
      ],
      "source": [
        "DIAGRAM_PROMPT = \"\"\"\n",
        "Analyze this architecture diagram.\n",
        "\n",
        "1. Identify all components (Users, Servers, Databases, Gateways, Load Balancers, etc.).\n",
        "2. Identify the connections and data flows between them.\n",
        "3. Identify trust boundaries (e.g., VPCs, Public/Private subnets, DMZs).\n",
        "\n",
        "Return ONLY a valid JSON object structured as:\n",
        "{\n",
        "  \"model\": \"model_name\",\n",
        "  \"components\": [{\"id\": \"unique_id\", \"type\": \"ComponentType\", \"name\": \"Display Name\", \"bbox\": [x_min, y_min, x_max, y_max]}],\n",
        "  \"connections\": [{\"from\": \"source_id\", \"to\": \"target_id\", \"protocol\": \"HTTPS/HTTP/TCP/etc\"}],\n",
        "  \"boundaries\": [\"boundary name 1\", \"boundary name 2\"]\n",
        "}\n",
        "\n",
        "Important:\n",
        "- Each component must have a unique id\n",
        "- Include bbox for each component: [x_min, y_min, x_max, y_max] as fractions 0.0-1.0 of image dimensions. Origin (0,0) top-left. x=horizontal, y=vertical. Example: top-left quarter = [0, 0, 0.25, 0.25]. Be precise: tight bounding box around each component.\n",
        "- Use descriptive component types (User, Server, Database, Gateway, LoadBalancer, Cache, Queue, API, Service)\n",
        "- Include the communication protocol for each connection when visible\n",
        "\"\"\"\n",
        "\n",
        "print(\"Prompt (primeiras linhas):\")\n",
        "print(DIAGRAM_PROMPT[:350], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Parser de JSON da resposta da LLM\n",
        "\n",
        "A LLM pode retornar texto com markdown (```json ... ```) ou JSON solto. Extraímos o objeto JSON válido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK  Parser de JSON definido\n"
          ]
        }
      ],
      "source": [
        "def parse_json_from_llm(text: str, service_name: str = \"LLM\") -> dict:\n",
        "    \"\"\"Extrai e parseia JSON da resposta da LLM.\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return {\"error\": \"Empty response\", \"error_type\": \"empty\", \"service\": service_name}\n",
        "    text = text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "    for start, end in [(\"{\", \"}\"), (\"[\", \"]\")]:\n",
        "        idx = text.find(start)\n",
        "        if idx != -1:\n",
        "            depth = 0\n",
        "            for i, c in enumerate(text[idx:], idx):\n",
        "                if c == start:\n",
        "                    depth += 1\n",
        "                elif c == end:\n",
        "                    depth -= 1\n",
        "                    if depth == 0:\n",
        "                        try:\n",
        "                            return json.loads(text[idx : i + 1])\n",
        "                        except json.JSONDecodeError:\n",
        "                            pass\n",
        "    match = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)```\", text)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1).strip())\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    return {\"error\": \"Invalid JSON response\", \"error_type\": \"invalid_json\", \"service\": service_name}\n",
        "\n",
        "print(\"OK  Parser de JSON definido\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Cache (evitar chamadas repetidas)\n",
        "\n",
        "Cache em SQLite por (prompt + hash da imagem). Economiza créditos ao reprocessar a mesma imagem.\n",
        "\n",
        "> ⚠️ **Nota:** Usamos SQLite no notebook para simplicidade. Em produção, o cache final será **Redis**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK  Cache SQLite: .langchain_cache.db\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "\n",
        "CACHE_DB = Path(\".langchain_cache.db\")\n",
        "if not CACHE_DB.exists():\n",
        "    CACHE_DB = Path(\"notebooks/.langchain_cache.db\")\n",
        "\n",
        "def _init_db():\n",
        "    with sqlite3.connect(CACHE_DB) as c:\n",
        "        c.execute(\"CREATE TABLE IF NOT EXISTS llm_cache (k TEXT PRIMARY KEY, v TEXT)\")\n",
        "\n",
        "def cache_key(prefix: str, prompt: str, image_bytes: bytes) -> str:\n",
        "    h = hashlib.sha256(image_bytes).hexdigest()[:16]\n",
        "    return f\"{prefix}_{hashlib.sha256((prompt + h).encode()).hexdigest()[:24]}\"\n",
        "\n",
        "def cache_get(prefix: str, prompt: str, image_bytes: bytes) -> dict | None:\n",
        "    _init_db()\n",
        "    key = cache_key(prefix, prompt, image_bytes)\n",
        "    with sqlite3.connect(CACHE_DB) as c:\n",
        "        row = c.execute(\"SELECT v FROM llm_cache WHERE k = ?\", (key,)).fetchone()\n",
        "    return json.loads(row[0]) if row else None\n",
        "\n",
        "def cache_set(prefix: str, value: dict, prompt: str, image_bytes: bytes) -> None:\n",
        "    _init_db()\n",
        "    key = cache_key(prefix, prompt, image_bytes)\n",
        "    with sqlite3.connect(CACHE_DB) as c:\n",
        "        c.execute(\"INSERT OR REPLACE INTO llm_cache (k, v) VALUES (?, ?)\", (key, json.dumps(value)))\n",
        "\n",
        "print(f\"OK  Cache SQLite: {CACHE_DB}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Invocação por LLM (Gemini, OpenAI, Qwen)\n",
        "\n",
        "Cada função envia a imagem em base64 + prompt para a API correspondente e retorna o JSON parseado ou um dict com `error`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK  Funções Gemini, OpenAI e Qwen (Ollama) definidas\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "except ImportError:\n",
        "    from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "def _image_mimetype(image_bytes: bytes) -> str:\n",
        "    if image_bytes[:8] == b\"\\x89PNG\\r\\n\\x1a\\n\":\n",
        "        return \"image/png\"\n",
        "    return \"image/jpeg\"\n",
        "\n",
        "async def invoke_gemini(prompt: str, image_bytes: bytes) -> dict:\n",
        "    if not CONFIG[\"google_api_key\"]:\n",
        "        return {\"error\": \"Gemini not configured\", \"service\": \"Gemini\"}\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=CONFIG[\"gemini_model\"],\n",
        "        temperature=CONFIG[\"temperature\"],\n",
        "        google_api_key=CONFIG[\"google_api_key\"],\n",
        "    )\n",
        "    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "    mime = _image_mimetype(image_bytes)\n",
        "    msg = HumanMessage(content=[\n",
        "        {\"type\": \"text\", \"text\": prompt},\n",
        "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{mime};base64,{b64}\"}},\n",
        "    ])\n",
        "    try:\n",
        "        r = await llm.ainvoke([msg])\n",
        "        return parse_json_from_llm(getattr(r, \"content\", str(r)), \"Gemini\")\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e), \"service\": \"Gemini\"}\n",
        "\n",
        "async def invoke_openai(prompt: str, image_bytes: bytes) -> dict:\n",
        "    if not CONFIG[\"openai_api_key\"]:\n",
        "        return {\"error\": \"OpenAI not configured\", \"service\": \"OpenAI\"}\n",
        "    llm = ChatOpenAI(\n",
        "        model=CONFIG[\"openai_model\"],\n",
        "        temperature=CONFIG[\"temperature\"],\n",
        "        api_key=CONFIG[\"openai_api_key\"],\n",
        "    )\n",
        "    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "    msg = HumanMessage(content=[\n",
        "        {\"type\": \"text\", \"text\": prompt},\n",
        "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64}\"}},\n",
        "    ])\n",
        "    try:\n",
        "        r = await llm.ainvoke([msg])\n",
        "        return parse_json_from_llm(getattr(r, \"content\", str(r)), \"OpenAI\")\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e), \"service\": \"OpenAI\"}\n",
        "\n",
        "async def invoke_ollama(prompt: str, image_bytes: bytes, model: str | None = None) -> dict:\n",
        "    \"\"\"Invoca Qwen via Ollama (LLM local de emergência).\"\"\"\n",
        "    model = model or CONFIG[\"ollama_model\"]\n",
        "    llm = ChatOllama(\n",
        "        model=model,\n",
        "        base_url=CONFIG[\"ollama_base_url\"],\n",
        "        client_kwargs={\"timeout\": CONFIG.get(\"ollama_timeout\", 60)},\n",
        "    )\n",
        "    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "    mime = _image_mimetype(image_bytes)\n",
        "    msg = HumanMessage(content=[\n",
        "        {\"type\": \"text\", \"text\": prompt},\n",
        "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{mime};base64,{b64}\"}},\n",
        "    ])\n",
        "    try:\n",
        "        r = await llm.ainvoke([msg])\n",
        "        return parse_json_from_llm(getattr(r, \"content\", str(r)), \"Ollama\")\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e), \"service\": \"Ollama\"}\n",
        "\n",
        "print(\"OK  Funções Gemini, OpenAI e Qwen (Ollama) definidas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Execução dos modelos (comparativo + fallback)\n",
        "\n",
        "- **run_all_models:** executa os 3 modelos em paralelo para comparativo.\n",
        "- **run_with_fallback:** tenta Gemini → OpenAI → Qwen em sequência; retorna o primeiro sucesso (simula produção)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK  run_all_models, run_with_fallback e metrics_from_result definidos\n"
          ]
        }
      ],
      "source": [
        "def is_valid_result(result: dict) -> bool:\n",
        "    if not isinstance(result, dict) or \"error\" in result:\n",
        "        return False\n",
        "    comps = result.get(\"components\", [])\n",
        "    return isinstance(comps, list)\n",
        "\n",
        "async def run_single_model(name: str, image_bytes: bytes) -> tuple[str, dict]:\n",
        "    if name.startswith(\"Ollama/\"):\n",
        "        model = name.split(\"/\", 1)[1]\n",
        "        cache_key_ollama = f\"ollama_{model}_diagram\"\n",
        "        cached = cache_get(cache_key_ollama, DIAGRAM_PROMPT, image_bytes)\n",
        "        if cached is not None:\n",
        "            res = cached\n",
        "        else:\n",
        "            res = await invoke_ollama(DIAGRAM_PROMPT, image_bytes, model=model)\n",
        "            if \"error\" not in res:\n",
        "                cache_set(cache_key_ollama, res, DIAGRAM_PROMPT, image_bytes)\n",
        "    elif name == \"Gemini\":\n",
        "        cached = cache_get(\"gemini_diagram\", DIAGRAM_PROMPT, image_bytes)\n",
        "        if cached is not None:\n",
        "            res = cached\n",
        "        else:\n",
        "            res = await invoke_gemini(DIAGRAM_PROMPT, image_bytes)\n",
        "            if \"error\" not in res:\n",
        "                cache_set(\"gemini_diagram\", res, DIAGRAM_PROMPT, image_bytes)\n",
        "    elif name == \"OpenAI\":\n",
        "        cached = cache_get(\"openai_diagram\", DIAGRAM_PROMPT, image_bytes)\n",
        "        if cached is not None:\n",
        "            res = cached\n",
        "        else:\n",
        "            res = await invoke_openai(DIAGRAM_PROMPT, image_bytes)\n",
        "            if \"error\" not in res:\n",
        "                cache_set(\"openai_diagram\", res, DIAGRAM_PROMPT, image_bytes)\n",
        "    else:\n",
        "        res = {\"error\": f\"Unknown model {name}\"}\n",
        "    return (name, res)\n",
        "\n",
        "async def run_all_models(image_bytes: bytes) -> dict[str, dict]:\n",
        "    \"\"\"Executa os 3 modelos em paralelo. Retorna {modelo: resultado} para comparativo.\"\"\"\n",
        "    tasks = [run_single_model(m, image_bytes) for m in MODELS_TO_RUN]\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "    out = {}\n",
        "    for name, r in zip(MODELS_TO_RUN, results):\n",
        "        if isinstance(r, Exception):\n",
        "            out[name] = {\"error\": str(r)}\n",
        "        else:\n",
        "            _, res = r\n",
        "            out[name] = res\n",
        "    return out\n",
        "\n",
        "\n",
        "async def run_with_fallback(image_bytes: bytes) -> tuple[str, dict]:\n",
        "    \"\"\"Tenta Gemini → OpenAI → Qwen em sequência. Retorna (modelo_usado, resultado).\"\"\"\n",
        "    for name in MODELS_TO_RUN:\n",
        "        _, res = await run_single_model(name, image_bytes)\n",
        "        if is_valid_result(res):\n",
        "            return (name, res)\n",
        "    return (MODELS_TO_RUN[-1], {\"error\": \"Todos os modelos falharam\"})\n",
        "\n",
        "\n",
        "def metrics_from_result(result: dict) -> dict:\n",
        "    \"\"\"Extrai métricas de um resultado para comparativo.\"\"\"\n",
        "    if result.get(\"error\"):\n",
        "        return {\"ok\": False, \"components\": 0, \"connections\": 0, \"boundaries\": 0, \"error\": str(result[\"error\"])[:50]}\n",
        "    comps = result.get(\"components\", []) or []\n",
        "    conns = result.get(\"connections\", []) or []\n",
        "    bnds = result.get(\"boundaries\", []) or []\n",
        "    n_bbox = sum(1 for c in comps if c.get(\"bbox\") and len(c.get(\"bbox\", [])) >= 4)\n",
        "    return {\n",
        "        \"ok\": True,\n",
        "        \"components\": len(comps),\n",
        "        \"connections\": len(conns),\n",
        "        \"boundaries\": len(bnds),\n",
        "        \"bbox_valid\": n_bbox,\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"OK  run_all_models, run_with_fallback e metrics_from_result definidos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Teste de fallback e métricas comparativas\n",
        "\n",
        "Primeiro testamos o fluxo de fallback em sequência. Depois, executamos os 3 modelos em paralelo e exibimos a tabela de métricas (componentes, conexões, boundaries, bbox válidos) para comparar qual performa melhor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Teste de fallback (Gemini → OpenAI → Qwen):\n",
            "   OK diagram01.png: usado=OpenAI, componentes=19, conexões=11\n",
            "   OK diagram02.png: usado=OpenAI, componentes=9, conexões=10\n",
            "\n",
            "2. Executando 3 modelos em paralelo...\n",
            "\n",
            "3. Métricas comparativas (qual performa melhor):\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "   diagram01.png:\n",
            "      Gemini               | ERRO: Error calling model 'gemini-1.5-pro' (NOT_FOU\n",
            "      OpenAI               | comp=19 | conn=11 | bbox=19\n",
            "      Ollama/qwen2.5vl     | comp= 0 | conn= 0 | bbox= 0\n",
            "\n",
            "   diagram02.png:\n",
            "      Gemini               | ERRO: Error calling model 'gemini-1.5-pro' (NOT_FOU\n",
            "      OpenAI               | comp= 9 | conn=10 | bbox= 9\n",
            "      Ollama/qwen2.5vl     | comp= 0 | conn= 0 | bbox= 0\n",
            "--------------------------------------------------------------------------------\n",
            "Executando 3 modelos em 2 imagens (paralelo)...\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# ---- Teste de fallback (sequência) ----\n",
        "test_images = [\"diagram01.png\", \"diagram02.png\"]\n",
        "paths = [(n, ASSETS_DIR / n) for n in test_images if (ASSETS_DIR / n).exists()]\n",
        "print(\"1. Teste de fallback (Gemini → OpenAI → Qwen):\")\n",
        "for name, path in paths:\n",
        "    used, res = await run_with_fallback(path.read_bytes())\n",
        "    m = metrics_from_result(res)\n",
        "    status = \"OK \" if m.get(\"ok\") else \"fail\"\n",
        "    print(f\"   {status} {name}: usado={used}, componentes={m.get('components', 0)}, conexões={m.get('connections', 0)}\")\n",
        "\n",
        "# ---- Execução paralela dos 3 modelos ----\n",
        "print(f\"\\n2. Executando {len(MODELS_TO_RUN)} modelos em paralelo...\")\n",
        "async def _run_one_image(name: str, path: Path):\n",
        "    return (name, await run_all_models(path.read_bytes()))\n",
        "gathered = await asyncio.gather(*[_run_one_image(n, p) for n, p in paths])\n",
        "all_results = {n: r for n, r in gathered}\n",
        "\n",
        "# ---- Tabela de métricas comparativas ----\n",
        "print(\"\\n3. Métricas comparativas (qual performa melhor):\")\n",
        "print(\"-\" * 80)\n",
        "for img_name in all_results:\n",
        "    print(f\"\\n   {img_name}:\")\n",
        "    for model in MODELS_TO_RUN:\n",
        "        r = all_results[img_name].get(model, {})\n",
        "        m = metrics_from_result(r)\n",
        "        if m.get(\"ok\"):\n",
        "            print(f\"      {model:20} | comp={m['components']:2} | conn={m['connections']:2} | bbox={m.get('bbox_valid', 0):2}\")\n",
        "        else:\n",
        "            print(f\"      {model:20} | ERRO: {m.get('error', '')[:45]}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ---- 4. Grid visual (bbox por modelo) ----\n",
        "def _bbox_to_pixels(bbox, w, h):\n",
        "    \"\"\"Converte bbox para (x1,y1,x2,y2) em pixels. Precisão: normalizado 0-1, ou % 0-100, ou pixels.\"\"\"\n",
        "    if not bbox or len(bbox) < 4:\n",
        "        return None\n",
        "    try:\n",
        "        vals = [float(x) for x in bbox[:4]]\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "    mx = max(vals)\n",
        "    if mx <= 1.0 and min(vals) >= 0:\n",
        "        x1, y1, x2, y2 = vals[0] * w, vals[1] * h, vals[2] * w, vals[3] * h\n",
        "    elif mx <= 100:\n",
        "        x1, y1, x2, y2 = vals[0]/100*w, vals[1]/100*h, vals[2]/100*w, vals[3]/100*h\n",
        "    else:\n",
        "        x1, y1, x2, y2 = vals[0], vals[1], vals[2], vals[3]\n",
        "    x1, x2 = max(0, min(x1, x2)), min(w, max(x1, x2))\n",
        "    y1, y2 = max(0, min(y1, y2)), min(h, max(y1, y2))\n",
        "    if x2 - x1 < 2 or y2 - y1 < 2:\n",
        "        return None\n",
        "    return (int(x1), int(y1), int(x2), int(y2))\n",
        "\n",
        "def plot_llm_result(img_array, result, title, model_name=\"\", ax=None):\n",
        "    \"\"\"Desenha bbox na imagem. ax=None cria figura nova.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    ax.imshow(img_array)\n",
        "    h, w = img_array.shape[:2]\n",
        "    comps = result.get(\"components\", []) if isinstance(result, dict) else []\n",
        "    n_bbox = 0\n",
        "    comps_sem_bbox = []\n",
        "    for c in comps:\n",
        "        pix = _bbox_to_pixels(c.get(\"bbox\"), w, h)\n",
        "        if pix:\n",
        "            x1, y1, x2, y2 = pix\n",
        "            label = f\"{c.get('type', '')} {c.get('name', '')}\".strip()[:30]\n",
        "            ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=\"lime\", linewidth=2))\n",
        "            ax.text(x1, max(0, y1 - 5), label, color=\"white\", fontsize=6, bbox=dict(facecolor=\"green\", alpha=0.8))\n",
        "            n_bbox += 1\n",
        "        else:\n",
        "            comps_sem_bbox.append(c)\n",
        "    if comps_sem_bbox:\n",
        "        ncol = min(4, len(comps_sem_bbox))\n",
        "        nrow = max(1, (len(comps_sem_bbox) + ncol - 1) // ncol)\n",
        "        pad = 20\n",
        "        cell_w = max(60, (w - 2 * pad) // ncol)\n",
        "        cell_h = max(40, (h - 2 * pad) // nrow)\n",
        "        for i, c in enumerate(comps_sem_bbox):\n",
        "            col, row = i % ncol, i // ncol\n",
        "            x1 = pad + col * cell_w + 5\n",
        "            y1 = pad + row * cell_h + 5\n",
        "            x2 = x1 + cell_w - 10\n",
        "            y2 = y1 + cell_h - 10\n",
        "            label = f\"{c.get('type', '')} {c.get('name', '')}\".strip()[:25]\n",
        "            ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=\"lime\", linewidth=2))\n",
        "            ax.text(x1, max(0, y1 - 5), label, color=\"white\", fontsize=6, bbox=dict(facecolor=\"green\", alpha=0.8))\n",
        "    ax.axis(\"off\")\n",
        "    err = \" (erro)\" if result.get(\"error\") else \"\"\n",
        "    sub = f\"{n_bbox} bbox\" if n_bbox else \"grid\"\n",
        "    ax.set_title(f\"{model_name} – {len(comps)} comp. {sub}{err}\")\n",
        "    if ax is None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "test_images = [\"diagram01.png\", \"diagram02.png\"]\n",
        "paths = [(n, ASSETS_DIR / n) for n in test_images if (ASSETS_DIR / n).exists()]\n",
        "print(f\"Executando {len(MODELS_TO_RUN)} modelos em {len(paths)} imagens (paralelo)...\")\n",
        "async def _run_one_image(name: str, path: Path):\n",
        "    return (name, await run_all_models(path.read_bytes()))\n",
        "tasks = [_run_one_image(n, p) for n, p in paths]\n",
        "gathered = await asyncio.gather(*tasks)\n",
        "all_results = {n: r for n, r in gathered}\n",
        "for name in all_results:\n",
        "    res = all_results[name]\n",
        "    ok = [m for m, r in res.items() if not r.get(\"error\")]\n",
        "    err = [(m, r.get(\"error\", \"\")[:60]) for m, r in res.items() if r.get(\"error\")]\n",
        "    print(f\"  OK {name}: {len(ok)} OK, {len(err)} erro(s)\")\n",
        "    for m, e in err:\n",
        "        print(f\"      ⚠ {m}: {e}...\")\n",
        "\n",
        "img_names = list(all_results.keys())\n",
        "nrows, ncols = len(img_names), len(MODELS_TO_RUN)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(4 * max(1, ncols), 4 * nrows))\n",
        "if nrows == 1 and ncols > 1:\n",
        "    axes = [axes]\n",
        "elif ncols == 1:\n",
        "    axes = [[a] for a in (axes if nrows > 1 else [axes])]\n",
        "for row, img_name in enumerate(img_names):\n",
        "    path = ASSETS_DIR / img_name\n",
        "    img_arr = np.array(Image.open(path).convert(\"RGB\"))\n",
        "    res = all_results.get(img_name, {})\n",
        "    for col, model_name in enumerate(MODELS_TO_RUN):\n",
        "        ax = axes[row][col]\n",
        "        plot_llm_result(img_arr, res.get(model_name, {}), img_name, model_name, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (threat-modeling-ai)",
      "language": "python",
      "name": "threat-modeling-ai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
